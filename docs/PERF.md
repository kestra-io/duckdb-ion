# Performance Baseline (read_ion vs read_json)

## Goal
Establish a repeatable baseline comparing `read_ion` to `read_json` on equivalent data and queries, without changing implementation yet.

## Step 1: Prepare comparable datasets
- Use the same schema and record count for Ion (newline‑delimited structs) and JSONL.
- If needed, export from DuckDB once and convert:
  - `COPY (SELECT * FROM my_table) TO 'data.ion' (FORMAT ION);`
  - `COPY (SELECT * FROM my_table) TO 'data.jsonl' (FORMAT JSON);`
- Or use the helper script to generate both:
  - `ROWS=100000 OUT_DIR=perf/data ./scripts/perf_generate.sh`
  - Re-run this script whenever serialization rules change.
  - Wide schema fixtures are also emitted as `data_wide.ion` and `data_wide.jsonl`.
  - When ion-c is available via vcpkg, the script also emits `data_binary.ion` and `data_wide_binary.ion`.

## Step 2: Run standardized queries
Pick at least three queries and run each on both formats:
1. Full scan count
   - `SELECT COUNT(*) FROM read_ion('data.ion');`
   - `SELECT COUNT(*) FROM read_json('data.jsonl');`
2. Projection
   - `SELECT col1, col2 FROM read_ion('data.ion');`
   - `SELECT col1, col2 FROM read_json('data.jsonl');`
3. Filter + aggregate
   - `SELECT col1, COUNT(*) FROM read_ion('data.ion') WHERE col2 > 10 GROUP BY col1;`
   - `SELECT col1, COUNT(*) FROM read_json('data.jsonl') WHERE col2 > 10 GROUP BY col1;`

## Step 3: Capture profiling output
Use DuckDB’s built‑in profiling:
```sql
PRAGMA enable_profiling='json';
PRAGMA profiling_output='ion_profile.json';
SELECT COUNT(*) FROM read_ion('data.ion');

PRAGMA profiling_output='json_profile.json';
SELECT COUNT(*) FROM read_json('data.jsonl');
```
Repeat for the other queries (use distinct profiling_output files).

## Step 4: Record baseline numbers
For each query, capture:
- wall time (from client or shell timing)
- rows/sec (if your client reports it)
- top operators from profiling output

## Step 5: Identify hotspots
Common hotspots to look for:
- per‑value conversions and casts
- recursive list/struct parsing
- string conversion for timestamps/decimals/blobs

## Step 6: Keep a log
Keep a simple table (spreadsheet or markdown) with:
- dataset size + row count
- query name
- read_ion time / read_json time
- notes on hotspots from profiling
You can run the standard set with:
- `DATA_DIR=perf/data OUT_DIR=perf/results ./scripts/perf_run.sh`

## Current Baseline (2M Rows)
The latest apples-to-apples comparison (same dataset, same query shape) from `perf/results/` shows Ion is still
slower than JSON. These were run against the newline-delimited datasets generated by `scripts/perf_generate.sh`.

Ion vs JSON ratios are computed as `Ion / JSON` using the profile JSON fields (`cpu_time`, `latency`).

| Query | Ion CPU | JSON CPU | CPU Ratio | Ion Latency | JSON Latency | Latency Ratio |
| --- | --- | --- | --- | --- | --- | --- |
| COUNT(*) | 0.945s | 0.295s | 3.20x | 0.949s | 0.054s | 17.4x |
| Project 3 cols | 1.210s | 0.345s | 3.50x | 1.221s | 0.060s | 20.2x |
| Filter + group | 1.199s | 0.341s | 3.51x | 1.201s | 0.059s | 20.5x |
| Wide schema (4 cols) | 7.460s | 2.027s | 3.68x | 7.478s | 0.285s | 26.2x |

Sources:
- `perf/results/ion_count.json`, `perf/results/json_count.json`
- `perf/results/ion_project.json`, `perf/results/json_project.json`
- `perf/results/ion_filter_agg.json`, `perf/results/json_filter_agg.json`
- `perf/results/ion_project_wide.json`, `perf/results/json_project_wide.json`

Notes:
- Total bytes read are similar between Ion and JSON; the gap is dominated by parsing and value materialization cost.
- Parallel newline-delimited runs are tracked separately (`*_nd_parallel.json`) and are not included above.
## Key Takeaways
- Text Ion remains ~3–4x slower than JSON on CPU and much slower on latency, pointing to per-row materialization overhead.
- Binary Ion is competitive (or faster) on CPU versus JSON text for comparable queries, but latency still lags.
- The largest absolute wins are from binary Ion on wide schemas, where text parsing overhead dominates.
- Parallelism helps throughput but does not close the per-row overhead gap for text Ion.

## Binary Ion Baseline (2M Rows)
Binary Ion files are generated from the text files using `scripts/ion_text_to_binary.cpp` (built in
`scripts/perf_generate.sh`). These runs compare `read_ion` on text vs binary Ion only.

| Query | Text CPU | Binary CPU | Speedup | Text Latency | Binary Latency | Speedup |
| --- | --- | --- | --- | --- | --- | --- |
| COUNT(*) | 0.945s | 0.120s | 7.88x | 0.949s | 0.122s | 7.81x |
| Project 3 cols | 1.210s | 0.442s | 2.74x | 1.221s | 0.449s | 2.72x |
| Wide COUNT(*) | 5.302s | 0.148s | 35.8x | 5.312s | 0.155s | 34.3x |
| Wide project (4 cols) | 7.460s | 1.581s | 4.72x | 7.478s | 1.595s | 4.69x |

Sources:
- `perf/results/ion_count.json`, `perf/results/ion_count_binary.json`
- `perf/results/ion_project.json`, `perf/results/ion_project_binary.json`
- `perf/results/ion_count_wide.json`, `perf/results/ion_count_wide_binary.json`
- `perf/results/ion_project_wide.json`, `perf/results/ion_project_wide_binary.json`

## Potential Improvements
- Enable projection pushdown to skip parsing unselected columns (mirrors `read_json` behavior).
- Batch-transform Ion values into vectors to reduce per-row `Value` allocations and casts.
- Cache field-to-column mappings beyond per-field SID lookup (e.g., stable field order heuristics).
- Consider `ION_READER_OPTIONS.skip_character_validation` for trusted data.
- Avoid string round-trips for decimals/timestamps by decoding Ion primitives directly.
- Parallelize newline-delimited scans to better utilize threads.

## Parallel Scan Notes
- Parallel scans currently only apply when `format='newline_delimited'` and the file is seekable.
- The perf suite now captures parallel JSON runs alongside Ion for apples-to-apples comparisons.
- Use `PRAGMA threads=<n>;` to force multiple threads during benchmarks.
